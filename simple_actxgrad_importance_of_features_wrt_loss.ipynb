{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chris/.cache/torch/hub/harvard-visionlab_open_ipcl_master\n",
      "/home/chris/miniconda3/envs/circuit_pruner/lib/python3.7/site-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "Tesla K40c with CUDA capability sm_35 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the Tesla K40c GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model, transform = torch.hub.load(\"harvard-visionlab/open_ipcl\", \"alexnetgn_supervised_ref13_augset1_1x\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pass imagenet data through the model, Im not sure how you access that data, but you need the official Imagenet dset on your file system, with 'train' & 'val' subfolders, with category-wise subfolders 'n01440764', 'n01443537' etc. point the following 'data_folder' variable to that imagenet path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/mnt/data/datasets/imagenet/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True, 'sampler':None} if 'cuda' in device else {}\n",
    "\n",
    "dset = ImageFolder(data_folder,transform=transform)\n",
    "dloader = DataLoader(dset,\n",
    "                     batch_size=256,\n",
    "                     shuffle=False,\n",
    "                     **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to 'score' the features in a layer by how much they might affect the model loss. We can approximate that as the average activationxgrad passing through a feature. Intuitively, this works because the gradient measures how much 'changing' the feature would affect the loss, while the activation size measures how much the feature would change (setting a high activation feature to 0 activation is a big change). Well use a 'hook' to save activation/gradient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alexnet_gn(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2), bias=False)\n",
       "    (1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_3): Sequential(\n",
       "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_block_4): Sequential(\n",
       "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_block_5): Sequential(\n",
       "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (ave_pool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (fc6): Sequential(\n",
       "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc7): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc8): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['', 'conv_block_1', 'conv_block_1.0', 'conv_block_1.1', 'conv_block_1.2', 'conv_block_1.3', 'conv_block_2', 'conv_block_2.0', 'conv_block_2.1', 'conv_block_2.2', 'conv_block_2.3', 'conv_block_3', 'conv_block_3.0', 'conv_block_3.1', 'conv_block_3.2', 'conv_block_4', 'conv_block_4.0', 'conv_block_4.1', 'conv_block_4.2', 'conv_block_5', 'conv_block_5.0', 'conv_block_5.1', 'conv_block_5.2', 'conv_block_5.3', 'ave_pool', 'fc6', 'fc6.0', 'fc6.1', 'fc6.2', 'fc7', 'fc7.0', 'fc7.1', 'fc7.2', 'fc8', 'fc8.0'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=9216, out_features=4096, bias=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#little trick for getting a dictionary with reference names for each module in your model, at all nestings\n",
    "layers = dict([*model.named_modules()])\n",
    "#so now we can reference modules with a string;\n",
    "print(layers.keys())\n",
    "layers['fc6.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=9216, out_features=4096, bias=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict([*model.named_modules()])['fc6.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from typing import Dict, Iterable, Callable\n",
    "\n",
    "class actgrad_extractor(nn.Module):\n",
    "    def __init__(self, model: nn.Module, layers: Iterable[str]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self.activations = {layer: None for layer in layers}\n",
    "        self.gradients = {layer: None for layer in layers}\n",
    "        hooks = {'forward':{},\n",
    "                 'backward':{}}   #saving hooks to variables lets us remove them later if we want\n",
    "        \n",
    "        for layer_id in layers:\n",
    "            layer = dict([*self.model.named_modules()])[layer_id]\n",
    "            hooks['forward'][layer_id] = layer.register_forward_hook(self.save_activations(layer_id)) #execute on forward pass\n",
    "            hooks['backward'][layer_id] = layer.register_backward_hook(self.save_gradients(layer_id))    #execute on backwards pass\n",
    "\n",
    "    def save_activations(self, layer_id: str) -> Callable:\n",
    "        def fn(module, input, output):  #register_hook expects to recieve a function with arguments like this\n",
    "            #output is what is return by the layer with dim (batch_dim x out_dim), sum across the batch dim\n",
    "            batch_summed_output = torch.sum(torch.abs(output),dim=0).detach().cpu()\n",
    "            if self.activations[layer_id] is None:\n",
    "                self.activations[layer_id] = batch_summed_output\n",
    "            else:\n",
    "                self.activations[layer_id] +=  batch_summed_output\n",
    "        return fn\n",
    "    \n",
    "    def save_gradients(self, layer_id: str) -> Callable:\n",
    "        def fn(module, grad_input, grad_output):\n",
    "            batch_summed_output = torch.sum(torch.abs(grad_output[0]),dim=0).detach().cpu() #grad_output is a tuple with 'device' as second item\n",
    "            if self.gradients[layer_id] is None:\n",
    "                self.gradients[layer_id] = batch_summed_output\n",
    "            else:\n",
    "                self.gradients[layer_id] +=  batch_summed_output \n",
    "        return fn\n",
    "    \n",
    "    def remove_all_hooks(self):\n",
    "        for hook in self.hooks['forward'].values():\n",
    "            hook.remove()\n",
    "        for hook in self.hooks['backward'].values():\n",
    "            hook.remove()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [\"conv_block_3.0\",\"fc6.0\"]\n",
    "\n",
    "model_actgrad_extractor = actgrad_extractor(model, layers=target_layers)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches: 196\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "remove_all_hooks() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2f923286de62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mactgrad_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_all_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: remove_all_hooks() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "iter_dataloader = iter(dloader)\n",
    "iters = len(iter_dataloader)  #if you want to test this out quickly just set this to a small number\n",
    "print('total batches: ' + str(iters)) \n",
    "for it in range(iters):\n",
    "    if it%10 == 0:\n",
    "        print(it)\n",
    "    inputs, target = next(iter_dataloader)\n",
    "    inputs = inputs.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    model.zero_grad()\n",
    "    \n",
    "    output = model(inputs)\n",
    "    \n",
    "    loss = criterion(output,target)\n",
    "    loss.backward()\n",
    "    \n",
    "\n",
    "actgrad_extractor.remove_all_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average by dividing result by length of dset\n",
    "activations = model_actgrad_extractor.activations\n",
    "gradients = model_actgrad_extractor.gradients\n",
    "\n",
    "for l in target_layers:\n",
    "    activations[l] /= len(dset)\n",
    "    gradients[l] /= len(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 13, 13])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "#scores are just actxgrad! Do with them what you want\n",
    "\n",
    "scores = {}\n",
    "for l in target_layers:\n",
    "    scores[l] = activations[l]*gradients[l]\n",
    "    print(scores[l].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuit_pruner",
   "language": "python",
   "name": "circuit_pruner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
